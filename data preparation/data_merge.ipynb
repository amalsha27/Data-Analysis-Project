{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffda53b0",
   "metadata": {},
   "source": [
    "### Data merge - Only pollutants O3, SO2, CO, NO2 and PM25 with target counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e2b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data written to daily_2025_multi_pollutant.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rx/gyp8p3vs42zfzk_hwhrnpcmw0000gn/T/ipykernel_5733/1729318645.py:84: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged.fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob, os\n",
    "\n",
    "# 1) Config\n",
    "data_dir = 'data_2025'\n",
    "target_states = ['06','36','48','17']\n",
    "target_counties = {\n",
    "    '06': ['037','075','073'],    # CA metros\n",
    "    '36': ['061','001'],          # NY metros\n",
    "    '48': ['201','113'],          # TX metros\n",
    "    '17': ['031']                 # IL metros\n",
    "}\n",
    "pollutants = {\n",
    "    '44201': 'O3',\n",
    "    '42401': 'SO2',\n",
    "    '42101': 'CO',\n",
    "    '42602': 'NO2',\n",
    "    '88101': 'PM25'\n",
    "}\n",
    "\n",
    "# Pre-compute valid (state,county) strings\n",
    "valid_pairs = {\n",
    "    f\"{st}_{cty}\"\n",
    "    for st, counties in target_counties.items()\n",
    "    for cty in counties\n",
    "}\n",
    "\n",
    "# 2) Chunked loader + filter\n",
    "def load_and_filter(path_zip, pollutant_name):\n",
    "    usecols = ['State Code','County Code','Site Num','Date Local','Arithmetic Mean']\n",
    "    reader = pd.read_csv(\n",
    "        path_zip,\n",
    "        usecols=usecols,\n",
    "        parse_dates=['Date Local'],\n",
    "        dtype={'State Code': str, 'County Code': str},\n",
    "        compression='zip',\n",
    "        chunksize=500_000,\n",
    "        low_memory=False\n",
    "    )\n",
    "    \n",
    "    chunks = []\n",
    "    for chunk in reader:\n",
    "        # zero-pad codes to match AQS formatting\n",
    "        chunk['State Code']  = chunk['State Code'].str.zfill(2)\n",
    "        chunk['County Code'] = chunk['County Code'].str.zfill(3)\n",
    "        \n",
    "        # keep only the target states\n",
    "        chunk = chunk[chunk['State Code'].isin(target_states)]\n",
    "        \n",
    "        # filter by valid (state,county)\n",
    "        chunk['pair'] = chunk['State Code'] + '_' + chunk['County Code']\n",
    "        chunk = chunk[chunk['pair'].isin(valid_pairs)]\n",
    "        chunk.drop(columns='pair', inplace=True)\n",
    "        \n",
    "        # rename pollutant column\n",
    "        chunk = chunk.rename(columns={'Arithmetic Mean': pollutant_name})\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "# 3) Merge all pollutants iteratively\n",
    "merged = None\n",
    "for code, name in pollutants.items():\n",
    "    zip_path = os.path.join(data_dir, f'daily_{code}_2025.zip')\n",
    "    df = load_and_filter(zip_path, name)\n",
    "    \n",
    "    if merged is None:\n",
    "        merged = df\n",
    "    else:\n",
    "        merged = merged.merge(\n",
    "            df,\n",
    "            on=['State Code','County Code','Site Num','Date Local'],\n",
    "            how='outer'\n",
    "        )\n",
    "    \n",
    "    del df\n",
    "\n",
    "# 4) Post-processing\n",
    "merged = merged.sort_values(\n",
    "    ['State Code','County Code','Site Num','Date Local']\n",
    ")\n",
    "merged.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# 5) Save\n",
    "merged.to_csv('daily_2025_multi_pollutant.csv', index=False)\n",
    "print(\"Merged data written to daily_2025_multi_pollutant.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a4422a",
   "metadata": {},
   "source": [
    "### Data merge - Pollutants O3, SO2, CO, NO2, PM25, meteorlogical data and air quality index with target counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3baafb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Loading daily_42101_2025.zip\n",
      "→ Loading daily_42401_2025.zip\n",
      "→ Loading daily_42602_2025.zip\n",
      "→ Loading daily_44201_2025.zip\n",
      "→ Loading daily_88101_2025.zip\n",
      "→ Loading daily_PRESS_2025.zip\n",
      "→ Loading daily_TEMP_2025.zip\n",
      "→ Loading daily_WIND_2025.zip\n",
      "→ Loading daily_RH_DP_2025.zip\n",
      "→ Loading daily AQI by county\n",
      "Finished! Output saved to daily_2025_full_merge.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rx/gyp8p3vs42zfzk_hwhrnpcmw0000gn/T/ipykernel_5733/649074219.py:127: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged.fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, glob\n",
    "\n",
    "# 1. Configuration\n",
    "data_dir = '/Users/amalshar/Desktop/USD/Fall 2025/Data Analysis/data preparation/data_2025'  \n",
    "output_csv = 'daily_2025_full_merge.csv'\n",
    "\n",
    "# Target geography\n",
    "target_states = ['06','36','48','17']\n",
    "target_counties = {\n",
    "    '06': ['037','075','073'],   # CA metros\n",
    "    '36': ['061','001'],         # NY metros\n",
    "    '48': ['201','113'],         # TX metros\n",
    "    '17': ['031']                # IL metros\n",
    "}\n",
    "valid_pairs = {\n",
    "    f\"{st}_{cty}\"\n",
    "    for st, counties in target_counties.items()\n",
    "    for cty in counties\n",
    "}\n",
    "\n",
    "# Mapping zip → parameter codes → output column names\n",
    "file_params = {\n",
    "    'daily_42101_2025.zip': {'42101':'CO'},\n",
    "    'daily_42401_2025.zip': {'42401':'SO2'},\n",
    "    'daily_42602_2025.zip': {'42602':'NO2'},\n",
    "    'daily_44201_2025.zip': {'44201':'O3'},\n",
    "    'daily_88101_2025.zip': {'88101':'PM25'},\n",
    "    'daily_PRESS_2025.zip':   {'64101':'BarometricPressure'},\n",
    "    'daily_TEMP_2025.zip':    {'62101':'Temperature'},\n",
    "    'daily_WIND_2025.zip':    {'62201':'WindResultant'},\n",
    "    'daily_RH_DP_2025.zip':   {'62202':'RelativeHumidity','62205':'DewPoint'}\n",
    "}\n",
    "\n",
    "# 2. Single‐code loader\n",
    "def load_single(zip_path, code, name):\n",
    "    usecols = ['State Code','County Code','Site Num','Date Local','Arithmetic Mean']\n",
    "    reader = pd.read_csv(\n",
    "        zip_path, usecols=usecols,\n",
    "        dtype={'State Code': str, 'County Code': str},\n",
    "        parse_dates=['Date Local'],\n",
    "        compression='zip', chunksize=300_000, low_memory=False\n",
    "    )\n",
    "    parts = []\n",
    "    for ch in reader:\n",
    "        ch['State Code']  = ch['State Code'].str.zfill(2)\n",
    "        ch['County Code'] = ch['County Code'].str.zfill(3)\n",
    "        ch = ch[ch['State Code'].isin(target_states)]\n",
    "        ch['pair'] = ch['State Code'] + '_' + ch['County Code']\n",
    "        ch = ch[ch['pair'].isin(valid_pairs)].drop(columns='pair')\n",
    "        ch = ch.rename(columns={'Arithmetic Mean': name})\n",
    "        parts.append(ch)\n",
    "    return pd.concat(parts, ignore_index=True)\n",
    "\n",
    "# 3. Multi‐code loader (for RH & DewPoint)\n",
    "def load_multi(zip_path, code_map):\n",
    "    usecols = ['State Code','County Code','Site Num','Date Local',\n",
    "               'Parameter Code','Arithmetic Mean']\n",
    "    reader = pd.read_csv(\n",
    "        zip_path, usecols=usecols,\n",
    "        dtype={'State Code': str, 'County Code': str, 'Parameter Code': str},\n",
    "        parse_dates=['Date Local'],\n",
    "        compression='zip', chunksize=300_000, low_memory=False\n",
    "    )\n",
    "    parts = []\n",
    "    keep_codes = set(code_map.keys())\n",
    "    for ch in reader:\n",
    "        ch['State Code']  = ch['State Code'].str.zfill(2)\n",
    "        ch['County Code'] = ch['County Code'].str.zfill(3)\n",
    "        ch = ch[ch['State Code'].isin(target_states)]\n",
    "        ch['pair'] = ch['State Code'] + '_' + ch['County Code']\n",
    "        ch = ch[ch['pair'].isin(valid_pairs)]\n",
    "        ch = ch[ch['Parameter Code'].isin(keep_codes)]\n",
    "        # pivot Parameter Code → column\n",
    "        piv = ch.pivot_table(\n",
    "            index=['State Code','County Code','Site Num','Date Local'],\n",
    "            columns='Parameter Code',\n",
    "            values='Arithmetic Mean',\n",
    "            aggfunc='first'\n",
    "        )\n",
    "        piv = piv.rename(columns=code_map).reset_index()\n",
    "        parts.append(piv)\n",
    "    return pd.concat(parts, ignore_index=True)\n",
    "\n",
    "# 4. Iterate and merge all\n",
    "merged = None\n",
    "for fname, cmap in file_params.items():\n",
    "    path = os.path.join(data_dir, fname)\n",
    "    print(f\"→ Loading {fname}\")\n",
    "    if len(cmap) == 1:\n",
    "        code, col = next(iter(cmap.items()))\n",
    "        df = load_single(path, code, col)\n",
    "    else:\n",
    "        df = load_multi(path, cmap)\n",
    "    if merged is None:\n",
    "        merged = df\n",
    "    else:\n",
    "        merged = merged.merge(\n",
    "            df,\n",
    "            on=['State Code','County Code','Site Num','Date Local'],\n",
    "            how='outer'\n",
    "        )\n",
    "    del df\n",
    "\n",
    "# 5. Merge daily AQI by county (no Site Num)\n",
    "aqi_zip = os.path.join(data_dir, 'daily_aqi_by_county_2025.zip')\n",
    "print(\"→ Loading daily AQI by county\")\n",
    "aqi = pd.read_csv(\n",
    "    aqi_zip,\n",
    "    dtype={'State Code': str, 'County Code': str},\n",
    "    parse_dates=['Date'],\n",
    "    compression='zip',\n",
    "    low_memory=False\n",
    ")\n",
    "aqi['State Code']  = aqi['State Code'].str.zfill(2)\n",
    "aqi['County Code'] = aqi['County Code'].str.zfill(3)\n",
    "aqi = aqi[aqi['State Code'].isin(target_states)]\n",
    "aqi = aqi.rename(columns={'Date':'Date Local','AQI':'DailyAQI'})\n",
    "merged = merged.merge(\n",
    "    aqi[['State Code','County Code','Date Local','DailyAQI']],\n",
    "    on=['State Code','County Code','Date Local'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 6. Final cleanup & save\n",
    "merged = merged.sort_values(['State Code','County Code','Site Num','Date Local'])\n",
    "merged.fillna(method='ffill', inplace=True)\n",
    "merged.to_csv(output_csv, index=False)\n",
    "print(f\"Finished! Output saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c8563c",
   "metadata": {},
   "source": [
    "### Data merge - Pollutants O3, SO2, CO, NO2, PM25, meteorlogical data and air quality index with all counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c38e737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading daily_42101_2025.zip …\n",
      "Loading daily_42401_2025.zip …\n",
      "Loading daily_42602_2025.zip …\n",
      "Loading daily_44201_2025.zip …\n",
      "Loading daily_88101_2025.zip …\n",
      "Loading daily_PRESS_2025.zip …\n",
      "Loading daily_TEMP_2025.zip …\n",
      "Loading daily_WIND_2025.zip …\n",
      "Loading daily_RH_DP_2025.zip …\n",
      "Loading daily AQI by county …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rx/gyp8p3vs42zfzk_hwhrnpcmw0000gn/T/ipykernel_5733/3400328356.py:121: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All counties in ['06', '36', '48', '17'] merged into daily_2025_full_merge_all_counties.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. Configuration\n",
    "data_dir    = '/Users/amalshar/Desktop/USD/Fall 2025/Data Analysis/data preparation/data_2025'  \n",
    "output_csv  = 'daily_2025_full_merge_all_counties.csv'\n",
    "\n",
    "# keep only these states, but include all counties\n",
    "target_states = ['06','36','48','17']\n",
    "\n",
    "# map each ZIP filename to its parameter codes → output column names\n",
    "file_params = {\n",
    "    'daily_42101_2025.zip': {'42101':'CO'},\n",
    "    'daily_42401_2025.zip': {'42401':'SO2'},\n",
    "    'daily_42602_2025.zip': {'42602':'NO2'},\n",
    "    'daily_44201_2025.zip': {'44201':'O3'},\n",
    "    'daily_88101_2025.zip': {'88101':'PM25'},\n",
    "    'daily_PRESS_2025.zip':  {'64101':'BarometricPressure'},\n",
    "    'daily_TEMP_2025.zip':   {'62101':'Temperature'},\n",
    "    'daily_WIND_2025.zip':   {'62201':'WindResultant'},\n",
    "    'daily_RH_DP_2025.zip':  {'62202':'RelativeHumidity','62205':'DewPoint'}\n",
    "}\n",
    "\n",
    "# 2. Loader for single-code files\n",
    "def load_single(zip_path, code, name):\n",
    "    usecols = ['State Code','County Code','Site Num','Date Local','Arithmetic Mean']\n",
    "    reader = pd.read_csv(\n",
    "        zip_path, usecols=usecols,\n",
    "        dtype={'State Code': str, 'County Code': str},\n",
    "        parse_dates=['Date Local'],\n",
    "        compression='zip', chunksize=300_000, low_memory=False\n",
    "    )\n",
    "    parts = []\n",
    "    for chunk in reader:\n",
    "        # zero-pad to match AQS formatting\n",
    "        chunk['State Code']  = chunk['State Code'].str.zfill(2)\n",
    "        chunk['County Code'] = chunk['County Code'].str.zfill(3)\n",
    "\n",
    "        # keep only our 4 states\n",
    "        chunk = chunk[chunk['State Code'].isin(target_states)]\n",
    "\n",
    "        # rename and collect\n",
    "        chunk = chunk.rename(columns={'Arithmetic Mean': name})\n",
    "        parts.append(chunk)\n",
    "\n",
    "    return pd.concat(parts, ignore_index=True)\n",
    "\n",
    "# 3. Loader for multi-code files (RH & DewPoint)\n",
    "def load_multi(zip_path, code_map):\n",
    "    usecols = ['State Code','County Code','Site Num','Date Local','Parameter Code','Arithmetic Mean']\n",
    "    reader = pd.read_csv(\n",
    "        zip_path, usecols=usecols,\n",
    "        dtype={'State Code': str, 'County Code': str, 'Parameter Code': str},\n",
    "        parse_dates=['Date Local'],\n",
    "        compression='zip', chunksize=300_000, low_memory=False\n",
    "    )\n",
    "    parts = []\n",
    "    keep_codes = set(code_map.keys())\n",
    "    for chunk in reader:\n",
    "        chunk['State Code']  = chunk['State Code'].str.zfill(2)\n",
    "        chunk['County Code'] = chunk['County Code'].str.zfill(3)\n",
    "        chunk = chunk[chunk['State Code'].isin(target_states)]\n",
    "        chunk = chunk[chunk['Parameter Code'].isin(keep_codes)]\n",
    "\n",
    "        # pivot codes into columns\n",
    "        piv = chunk.pivot_table(\n",
    "            index=['State Code','County Code','Site Num','Date Local'],\n",
    "            columns='Parameter Code',\n",
    "            values='Arithmetic Mean',\n",
    "            aggfunc='first'\n",
    "        ).rename(columns=code_map).reset_index()\n",
    "\n",
    "        parts.append(piv)\n",
    "\n",
    "    return pd.concat(parts, ignore_index=True)\n",
    "\n",
    "# 4. Iteratively load & merge all parameter files\n",
    "merged = None\n",
    "for fname, cmap in file_params.items():\n",
    "    path = os.path.join(data_dir, fname)\n",
    "    print(f\"Loading {fname} …\")\n",
    "\n",
    "    if len(cmap) == 1:\n",
    "        code, col = next(iter(cmap.items()))\n",
    "        df = load_single(path, code, col)\n",
    "    else:\n",
    "        df = load_multi(path, cmap)\n",
    "\n",
    "    if merged is None:\n",
    "        merged = df\n",
    "    else:\n",
    "        merged = merged.merge(\n",
    "            df,\n",
    "            on=['State Code','County Code','Site Num','Date Local'],\n",
    "            how='outer'\n",
    "        )\n",
    "    del df  \n",
    "\n",
    "# 5. Merge county-level Daily AQI (no Site Num)\n",
    "aqi_zip = os.path.join(data_dir, 'daily_aqi_by_county_2025.zip')\n",
    "print(\"Loading daily AQI by county …\")\n",
    "aqi = pd.read_csv(\n",
    "    aqi_zip,\n",
    "    dtype={'State Code': str, 'County Code': str},\n",
    "    parse_dates=['Date'],\n",
    "    compression='zip', low_memory=False\n",
    ")\n",
    "aqi['State Code']  = aqi['State Code'].str.zfill(2)\n",
    "aqi['County Code'] = aqi['County Code'].str.zfill(3)\n",
    "aqi = aqi[aqi['State Code'].isin(target_states)]\n",
    "aqi = aqi.rename(columns={'Date':'Date Local','AQI':'DailyAQI'})\n",
    "\n",
    "merged = merged.merge(\n",
    "    aqi[['State Code','County Code','Date Local','DailyAQI']],\n",
    "    on=['State Code','County Code','Date Local'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 6. Final cleanup & save\n",
    "merged = merged.sort_values(['State Code','County Code','Site Num','Date Local'])\n",
    "merged.fillna(method='ffill', inplace=True)\n",
    "merged.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"All counties in {target_states} merged into {output_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
